#!/usr/bin/env python3
"""
Yahoo ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦ Issue ç”¨ã® Markdown ã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
BeautifulSoup ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨ç„¡æ–™ç‰ˆï¼ˆAPI ä¸è¦ï¼‰
"""

import sys
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup


def fetch_html(url: str) -> str:
    """æŒ‡å®šURLã®HTMLã‚’å–å¾—"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    response = requests.get(url, headers=headers, timeout=15)
    response.raise_for_status()
    return response.text


def extract_news_from_html(html: str, category: str) -> List[Dict[str, str]]:
    """BeautifulSoupã‚’ä½¿ã£ã¦HTMLã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡º"""

    soup = BeautifulSoup(html, 'html.parser')
    articles = []

    # Yahoo ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œã—ã¦è¨˜äº‹ã‚’å–å¾—

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: <a> ã‚¿ã‚°ã§ href ã« '/articles/' ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®
    links = soup.find_all('a', href=lambda x: x and '/articles/' in x)

    seen_urls = set()

    for link in links:
        href = link.get('href', '')

        # å®Œå…¨ãªURLã«å¤‰æ›
        if href.startswith('/'):
            url = f'https://news.yahoo.co.jp{href}'
        elif not href.startswith('http'):
            continue
        else:
            url = href

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in seen_urls:
            continue

        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
        title = None

        # æ–¹æ³•1: aria-label å±æ€§
        if link.get('aria-label'):
            title = link.get('aria-label')

        # æ–¹æ³•2: ãƒªãƒ³ã‚¯å†…ã®ãƒ†ã‚­ã‚¹ãƒˆ
        if not title:
            title = link.get_text(strip=True)

        # æ–¹æ³•3: è¦ªè¦ç´ ã‹ã‚‰å–å¾—
        if not title or len(title) < 10:
            parent = link.find_parent(['li', 'div', 'article'])
            if parent:
                # è¦‹å‡ºã—ã‚¿ã‚°ã‚’æ¢ã™
                heading = parent.find(['h1', 'h2', 'h3', 'h4'])
                if heading:
                    title = heading.get_text(strip=True)

        # ã‚¿ã‚¤ãƒˆãƒ«ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿½åŠ 
        if title and len(title) >= 10 and url.startswith('https://news.yahoo.co.jp/articles/'):
            articles.append({
                'title': title,
                'url': url
            })
            seen_urls.add(url)

            # 8ä»¶å–å¾—ã—ãŸã‚‰çµ‚äº†
            if len(articles) >= 8:
                break

    # 8ä»¶ã«æº€ãŸãªã„å ´åˆã€åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è©¦è¡Œ
    if len(articles) < 8:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: data-cl-params å±æ€§ã‚’æŒã¤ãƒªãƒ³ã‚¯ï¼ˆYahooç‰¹æœ‰ã®å±æ€§ï¼‰
        additional_links = soup.find_all('a', attrs={'data-cl-params': True})

        for link in additional_links:
            if len(articles) >= 8:
                break

            href = link.get('href', '')

            if '/articles/' not in href:
                continue

            if href.startswith('/'):
                url = f'https://news.yahoo.co.jp{href}'
            else:
                url = href

            if url in seen_urls:
                continue

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = link.get_text(strip=True)

            if not title or len(title) < 10:
                continue

            articles.append({
                'title': title,
                'url': url
            })
            seen_urls.add(url)

    return articles[:8]  # æœ€å¤§8ä»¶


def generate_markdown(domestic_articles, world_articles, business_articles) -> str:
    """Issueç”¨ã®Markdownã‚’ç”Ÿæˆ"""

    now = datetime.now()
    date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    md = f"""# ğŸ“° Yahoo ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚ - {date_str}

> è‡ªå‹•å–å¾—æ—¥æ™‚: {now.strftime("%Y-%m-%d %H:%M:%S")}

---

## ğŸ  å›½å†…ãƒ‹ãƒ¥ãƒ¼ã‚¹

"""

    # å›½å†…ã‚¿ã‚¤ãƒˆãƒ«
    for i, article in enumerate(domestic_articles[:8], 1):
        md += f"{i}. {article['title']}\n"

    md += "\n## ğŸŒ å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n"

    # å›½éš›ã‚¿ã‚¤ãƒˆãƒ«
    for i, article in enumerate(world_articles[:8], 1):
        md += f"{i}. {article['title']}\n"

    md += "\n## ğŸ’¼ çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n"

    # çµŒæ¸ˆã‚¿ã‚¤ãƒˆãƒ«
    for i, article in enumerate(business_articles[:8], 1):
        md += f"{i}. {article['title']}\n"

    md += "\n---\n\n## ğŸ”— ãƒªãƒ³ã‚¯é›†\n\n### å›½å†…\n"

    # ãƒªãƒ³ã‚¯é›†
    for i, article in enumerate(domestic_articles[:8], 1):
        md += f"{i}. [{article['title']}]({article['url']})\n"

    md += "\n### å›½éš›\n"
    for i, article in enumerate(world_articles[:8], 1):
        md += f"{i}. [{article['title']}]({article['url']})\n"

    md += "\n### çµŒæ¸ˆ\n"
    for i, article in enumerate(business_articles[:8], 1):
        md += f"{i}. [{article['title']}]({article['url']})\n"

    md += "\n---\n\n"
    md += "*ã“ã®Issueã¯ GitHub Actions ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼ˆå®Œå…¨ç„¡æ–™ç‰ˆï¼‰*\n"

    return md


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""

    # å„ã‚«ãƒ†ã‚´ãƒªã®URL
    categories = {
        'domestic': 'https://news.yahoo.co.jp/categories/domestic',
        'world': 'https://news.yahoo.co.jp/categories/world',
        'business': 'https://news.yahoo.co.jp/categories/business'
    }

    print("ğŸ“¡ Fetching Yahoo News (å®Œå…¨ç„¡æ–™ç‰ˆ)...")

    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    results = {}

    for category_key, url in categories.items():
        print(f"  - Fetching {category_key}...")
        try:
            html = fetch_html(url)
            articles = extract_news_from_html(html, category_key)
            results[category_key] = articles
            print(f"    âœ“ Found {len(articles)} articles")
        except Exception as e:
            print(f"    âœ— Error: {e}")
            import traceback
            traceback.print_exc()
            results[category_key] = []

    # è¨˜äº‹ãŒ1ä»¶ã‚‚å–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼
    total_articles = sum(len(v) for v in results.values())
    if total_articles == 0:
        print("\nâŒ Error: No articles found. Yahoo may have changed their HTML structure.")
        sys.exit(1)

    # Markdown ã‚’ç”Ÿæˆ
    print("\nğŸ“ Generating Markdown...")
    markdown = generate_markdown(
        results.get('domestic', []),
        results.get('world', []),
        results.get('business', [])
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = 'news_output.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown)

    print(f"âœ… News saved to {output_file}")
    print(f"\nTotal articles: {total_articles}")


if __name__ == '__main__':
    main()
